#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
pcap_rule_len_stats.py
- Read packets from pcap/pcapng
- Match packet against YAML rules (subset aligned to your rule schema)
- Compute descriptive stats of packet sizes for matched packets (overall + per-label)

Dependencies:
  pip install dpkt pyyaml numpy
"""

import argparse
import datetime as dt
import json
import math
import os
import sys
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import dpkt
import numpy as np
import yaml


DEFAULT_YAML = "/mnt/raid/luohaoran/cicids2018/SaP/pcapphaser/label_rules/cic2018_improved_rules_simplifed.yaml"


def eprint(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)


def parse_iso_to_epoch(s: str, tz_offset_hours: float = 0.0) -> float:
    """
    YAML time strings are usually naive ISO8601 (no timezone).
    We interpret them as UTC + tz_offset_hours (default 0).
    """
    # Handle "2018-03-01T14:09:48.354333"
    try:
        t = dt.datetime.fromisoformat(s)
    except ValueError:
        # Fallback: strip Z if present
        if s.endswith("Z"):
            t = dt.datetime.fromisoformat(s[:-1])
        else:
            raise

    # Treat as naive local with offset -> convert to UTC epoch
    # If tz_offset_hours = +8, it means YAML times are UTC+8.
    t_utc = t - dt.timedelta(hours=tz_offset_hours)
    return t_utc.replace(tzinfo=dt.timezone.utc).timestamp()


def ip_to_str(ip_bytes: bytes) -> str:
    if len(ip_bytes) == 4:
        return ".".join(str(b) for b in ip_bytes)
    if len(ip_bytes) == 16:
        import ipaddress
        return str(ipaddress.IPv6Address(ip_bytes))
    return ""


def get_packet_fields(ts: float, buf: bytes) -> Optional[Dict[str, Any]]:
    """
    Parse L2->L3->L4 and return basic fields used by rules.
    """
    try:
        eth = dpkt.ethernet.Ethernet(buf)
        ip = eth.data
    except Exception:
        return None

    fields: Dict[str, Any] = {
        "ts": ts,
        "src_ip": None,
        "dst_ip": None,
        "proto": None,
        "src_port": None,
        "dst_port": None,
        "tcp_rst": 0,
        "tcp_payload_len": 0,
        "caplen": len(buf),
        "iplen": None,
    }

    # IPv4 / IPv6
    if isinstance(ip, dpkt.ip.IP):
        fields["proto"] = ip.p
        fields["src_ip"] = ip_to_str(ip.src)
        fields["dst_ip"] = ip_to_str(ip.dst)
        fields["iplen"] = int(ip.len)
        l4 = ip.data
    elif isinstance(ip, dpkt.ip6.IP6):
        fields["proto"] = ip.nxt
        fields["src_ip"] = ip_to_str(ip.src)
        fields["dst_ip"] = ip_to_str(ip.dst)
        fields["iplen"] = int(ip.plen) + 40  # IPv6 payload + fixed header
        l4 = ip.data
    else:
        return None

    # TCP / UDP
    if isinstance(l4, dpkt.tcp.TCP):
        fields["src_port"] = int(l4.sport)
        fields["dst_port"] = int(l4.dport)
        flags = int(l4.flags)
        # TCP RST flag is 0x04
        fields["tcp_rst"] = 1 if (flags & dpkt.tcp.TH_RST) else 0
        fields["tcp_payload_len"] = len(l4.data) if l4.data else 0
    elif isinstance(l4, dpkt.udp.UDP):
        fields["src_port"] = int(l4.sport)
        fields["dst_port"] = int(l4.dport)
    else:
        # other L4: keep ports None
        pass

    return fields


@dataclass
class TimeWindow:
    ranges: List[Tuple[float, float]]  # list of [start_epoch, end_epoch], inclusive of start, inclusive of end


@dataclass
class Rule:
    label: str
    priority: int
    terms: List[Dict[str, Any]]  # original match terms
    time_window: Optional[TimeWindow] = None

    # Precompiled sets for faster evaluation
    src_ip_set: Optional[set] = None
    dst_ip_set: Optional[set] = None
    any_ip_set: Optional[set] = None

    # Optional exact comparisons
    dst_ip_eq: Optional[str] = None
    src_ip_eq: Optional[str] = None

    # Ports
    src_port_terms: List[Tuple[str, Any]] = None
    dst_port_terms: List[Tuple[str, Any]] = None

    # Flow-dependent terms (directional)
    zero_fwd_term: Optional[Tuple[str, bool]] = None
    total_fwd_len_term: Optional[Tuple[str, int]] = None
    bwd_rst_flags_term: Optional[Tuple[str, int]] = None

    def __post_init__(self):
        self.src_port_terms = []
        self.dst_port_terms = []


def compile_rules(yaml_path: str, tz_offset_hours: float) -> List[Rule]:
    with open(yaml_path, "r", encoding="utf-8") as f:
        y = yaml.safe_load(f)

    rules_raw = y.get("rules", [])
    compiled: List[Rule] = []

    for r in rules_raw:
        label = r.get("label", "UNKNOWN")
        priority = int(r.get("priority", 0))
        terms = r.get("match", []) or []

        rule = Rule(label=label, priority=priority, terms=terms)

        # parse terms
        for t in terms:
            field = t.get("field")
            op = t.get("op")
            val = t.get("value")

            if field == "time_window":
                if op == "range":
                    start, end = val
                    tw = [(parse_iso_to_epoch(start, tz_offset_hours),
                           parse_iso_to_epoch(end, tz_offset_hours))]
                elif op == "ranges":
                    tw = []
                    for start, end in val:
                        tw.append((parse_iso_to_epoch(start, tz_offset_hours),
                                   parse_iso_to_epoch(end, tz_offset_hours)))
                else:
                    raise ValueError(f"Unsupported time_window op: {op}")
                rule.time_window = TimeWindow(ranges=tw)

            elif field in ("src_ip", "dst_ip", "any_ip"):
                if op == "in":
                    s = set(val)
                    if field == "src_ip":
                        rule.src_ip_set = (rule.src_ip_set or set()) | s
                    elif field == "dst_ip":
                        rule.dst_ip_set = (rule.dst_ip_set or set()) | s
                    else:
                        rule.any_ip_set = (rule.any_ip_set or set()) | s
                elif op == "==":
                    if field == "src_ip":
                        rule.src_ip_eq = str(val)
                    elif field == "dst_ip":
                        rule.dst_ip_eq = str(val)
                    else:
                        # any_ip == X (rare)
                        rule.any_ip_set = (rule.any_ip_set or set()) | {str(val)}
                else:
                    # Keep as generic term (evaluate later)
                    pass

            elif field == "src_port":
                rule.src_port_terms.append((op, val))
            elif field == "dst_port":
                rule.dst_port_terms.append((op, val))

            elif field == "zero_fwd":
                # value is true/false
                rule.zero_fwd_term = (op, bool(val))
            elif field == "total_fwd_len":
                rule.total_fwd_len_term = (op, int(val))
            elif field == "bwd_rst_flags":
                rule.bwd_rst_flags_term = (op, int(val))
            else:
                # Unknown fields are allowed but will be evaluated only if we implement them.
                # For now, keep them in terms; generic evaluator will likely ignore and treat as non-match.
                pass

        compiled.append(rule)

    # Highest priority first (so "Attempted" can override base if desired)
    compiled.sort(key=lambda x: x.priority, reverse=True)
    return compiled


def cmp(op: str, a: Any, b: Any) -> bool:
    if op == "==":
        return a == b
    if op == "!=":
        return a != b
    if op == ">":
        return a > b
    if op == ">=":
        return a >= b
    if op == "<":
        return a < b
    if op == "<=":
        return a <= b
    if op == "in":
        return a in b
    raise ValueError(f"Unsupported op: {op}")


def in_time_window(ts: float, tw: Optional[TimeWindow]) -> bool:
    if tw is None:
        return True
    for start, end in tw.ranges:
        if start <= ts <= end:
            return True
    return False


def make_flow_key(fields: Dict[str, Any]) -> Optional[Tuple[str, int, str, int, int]]:
    """
    Directional 5-tuple key: (src_ip, src_port, dst_ip, dst_port, proto)
    Only meaningful if ports exist.
    """
    if fields["src_ip"] is None or fields["dst_ip"] is None:
        return None
    if fields["src_port"] is None or fields["dst_port"] is None:
        return None
    if fields["proto"] is None:
        return None
    return (fields["src_ip"], int(fields["src_port"]), fields["dst_ip"], int(fields["dst_port"]), int(fields["proto"]))


def reverse_flow_key(k: Tuple[str, int, str, int, int]) -> Tuple[str, int, str, int, int]:
    return (k[2], k[3], k[0], k[1], k[4])


def eval_rule(
    rule: Rule,
    fields: Dict[str, Any],
    payload_bytes_dir: Dict[Tuple[str, int, str, int, int], int],
    rst_pkts_dir: Dict[Tuple[str, int, str, int, int], int],
) -> bool:
    # time window first (cheap)
    if not in_time_window(fields["ts"], rule.time_window):
        return False

    src_ip = fields["src_ip"]
    dst_ip = fields["dst_ip"]
    if src_ip is None or dst_ip is None:
        return False

    # IP constraints
    if rule.src_ip_eq is not None and src_ip != rule.src_ip_eq:
        return False
    if rule.dst_ip_eq is not None and dst_ip != rule.dst_ip_eq:
        return False
    if rule.src_ip_set is not None and src_ip not in rule.src_ip_set:
        return False
    if rule.dst_ip_set is not None and dst_ip not in rule.dst_ip_set:
        return False
    if rule.any_ip_set is not None and (src_ip not in rule.any_ip_set and dst_ip not in rule.any_ip_set):
        return False

    # port constraints
    if rule.src_port_terms:
        sp = fields["src_port"]
        if sp is None:
            return False
        for op, val in rule.src_port_terms:
            if not cmp(op, int(sp), int(val)):
                return False

    if rule.dst_port_terms:
        dp = fields["dst_port"]
        if dp is None:
            return False
        for op, val in rule.dst_port_terms:
            if not cmp(op, int(dp), int(val)):
                return False

    # flow constraints (directional)
    fk = make_flow_key(fields)
    if (rule.zero_fwd_term or rule.total_fwd_len_term or rule.bwd_rst_flags_term) and fk is None:
        return False

    if rule.zero_fwd_term is not None:
        op, want = rule.zero_fwd_term
        fwd_bytes = int(payload_bytes_dir.get(fk, 0))
        got = (fwd_bytes == 0)
        if not cmp(op, got, want):
            return False

    if rule.total_fwd_len_term is not None:
        op, v = rule.total_fwd_len_term
        fwd_bytes = int(payload_bytes_dir.get(fk, 0))
        if not cmp(op, fwd_bytes, int(v)):
            return False

    if rule.bwd_rst_flags_term is not None:
        op, v = rule.bwd_rst_flags_term
        rk = reverse_flow_key(fk)
        bwd_rst = int(rst_pkts_dir.get(rk, 0))
        if not cmp(op, bwd_rst, int(v)):
            return False

    # If YAML has additional fields we didn't compile, we ignore them (treat as satisfied)
    return True


def compute_stats(arr: np.ndarray) -> Dict[str, Any]:
    """
    Compute a rich set of descriptive stats.
    """
    if arr.size == 0:
        return {"count": 0}

    x = arr.astype(np.float64)
    n = int(x.size)

    mean = float(x.mean())
    var = float(x.var(ddof=1)) if n > 1 else 0.0
    std = float(math.sqrt(var))
    mn = float(x.min())
    mx = float(x.max())
    med = float(np.median(x))
    q1 = float(np.quantile(x, 0.25))
    q3 = float(np.quantile(x, 0.75))
    iqr = float(q3 - q1)
    p90 = float(np.quantile(x, 0.90))
    p95 = float(np.quantile(x, 0.95))
    p99 = float(np.quantile(x, 0.99))

    mad = float(np.median(np.abs(x - med)))
    cv = float(std / mean) if mean != 0 else float("inf")

    # Skewness & kurtosis (excess), using unbiased-ish formulas when possible
    if n >= 3 and std > 0:
        m3 = float(np.mean((x - mean) ** 3))
        skew = float(m3 / (std ** 3))
    else:
        skew = 0.0

    if n >= 4 and std > 0:
        m4 = float(np.mean((x - mean) ** 4))
        kurt_excess = float(m4 / (std ** 4) - 3.0)
    else:
        kurt_excess = 0.0

    return {
        "count": n,
        "mean": mean,
        "var": var,
        "std": std,
        "min": mn,
        "q1": q1,
        "median": med,
        "q3": q3,
        "iqr": iqr,
        "p90": p90,
        "p95": p95,
        "p99": p99,
        "max": mx,
        "mad": mad,
        "cv": cv,
        "skew": skew,
        "kurtosis_excess": kurt_excess,
    }


def open_pcap(path: str):
    f = open(path, "rb")
    # Try pcapng first, then pcap
    try:
        r = dpkt.pcapng.Reader(f)
        return f, r
    except (ValueError, dpkt.dpkt.NeedData):
        f.seek(0)
        r = dpkt.pcap.Reader(f)
        return f, r


def main():
    ap = argparse.ArgumentParser(
        description="Compute packet-length stats for packets matching YAML rules in a PCAP."
    )
    ap.add_argument("pcap", help="Path to .pcap or .pcapng")
    ap.add_argument(
        "--yaml",
        default=DEFAULT_YAML,
        help=f"Path to rule YAML (default: {DEFAULT_YAML})",
    )
    ap.add_argument(
        "--metric",
        choices=["caplen", "iplen"],
        default="caplen",
        help="Packet size metric: caplen=len(pcap record) or iplen=IP total length",
    )
    ap.add_argument(
        "--tz-offset",
        type=float,
        default=0.0,
        help="Interpret YAML timestamps as UTC+offset hours. Default 0 (UTC).",
    )
    ap.add_argument(
        "--json",
        dest="json_out",
        action="store_true",
        help="Output JSON instead of human-readable table.",
    )
    ap.add_argument(
        "--max-packets",
        type=int,
        default=0,
        help="Optional cap on number of packets to read (0 means no limit).",
    )
    ap.add_argument(
        "--quiet",
        action="store_true",
        help="Less stderr progress output.",
    )
    args = ap.parse_args()

    if not os.path.isfile(args.pcap):
        eprint(f"[FATAL] pcap not found: {args.pcap}")
        sys.exit(2)
    if not os.path.isfile(args.yaml):
        eprint(f"[FATAL] yaml not found: {args.yaml}")
        sys.exit(2)

    rules = compile_rules(args.yaml, args.tz_offset)
    if not rules:
        eprint("[FATAL] No rules found in yaml.")
        sys.exit(2)

    # Pass 1: directional flow stats (payload bytes, rst packets)
    payload_bytes_dir: Dict[Tuple[str, int, str, int, int], int] = {}
    rst_pkts_dir: Dict[Tuple[str, int, str, int, int], int] = {}

    f, r = open_pcap(args.pcap)
    pkt_count = 0
    try:
        for ts, buf in r:
            pkt_count += 1
            if args.max_packets and pkt_count > args.max_packets:
                break
            fields = get_packet_fields(ts, buf)
            if fields is None:
                continue

            fk = make_flow_key(fields)
            if fk is None:
                continue

            # payload bytes: TCP only (udp payload len not used by your YAML fields)
            if fields["tcp_payload_len"] is not None:
                payload_bytes_dir[fk] = payload_bytes_dir.get(fk, 0) + int(fields["tcp_payload_len"])
            if fields["tcp_rst"]:
                rst_pkts_dir[fk] = rst_pkts_dir.get(fk, 0) + 1

            if (not args.quiet) and (pkt_count % 500000 == 0):
                eprint(f"[pass1] processed packets: {pkt_count}")
    finally:
        f.close()

    # Pass 2: match packets and collect lengths
    lengths_overall: List[int] = []
    lengths_by_label: Dict[str, List[int]] = {}

    f, r = open_pcap(args.pcap)
    pkt_count2 = 0
    matched = 0
    try:
        for ts, buf in r:
            pkt_count2 += 1
            if args.max_packets and pkt_count2 > args.max_packets:
                break
            fields = get_packet_fields(ts, buf)
            if fields is None:
                continue

            # Determine packet length metric
            if args.metric == "caplen":
                plen = int(fields["caplen"])
            else:
                plen = int(fields["iplen"]) if fields["iplen"] is not None else int(fields["caplen"])

            # Find first matching rule (priority-ordered)
            label = None
            for rule in rules:
                if eval_rule(rule, fields, payload_bytes_dir, rst_pkts_dir):
                    label = rule.label
                    break

            if label is None:
                continue

            matched += 1
            lengths_overall.append(plen)
            lengths_by_label.setdefault(label, []).append(plen)

            if (not args.quiet) and (matched % 200000 == 0):
                eprint(f"[pass2] matched packets: {matched}")

    finally:
        f.close()

    # Prepare results
    out: Dict[str, Any] = {
        "pcap": args.pcap,
        "yaml": args.yaml,
        "metric": args.metric,
        "tz_offset_hours": args.tz_offset,
        "packets_read": pkt_count2,
        "packets_matched": matched,
        "overall": compute_stats(np.array(lengths_overall, dtype=np.int64)),
        "by_label": {},
    }

    for label, lens in sorted(lengths_by_label.items(), key=lambda kv: len(kv[1]), reverse=True):
        out["by_label"][label] = compute_stats(np.array(lens, dtype=np.int64))

    if args.json_out:
        print(json.dumps(out, ensure_ascii=False, indent=2))
        return

    # Human-readable output
    def fmt(v):
        if isinstance(v, float):
            if math.isinf(v):
                return "inf"
            return f"{v:.4f}"
        return str(v)

    print(f"PCAP:   {out['pcap']}")
    print(f"YAML:   {out['yaml']}")
    print(f"Metric: {out['metric']}   (caplen=len(record), iplen=IP total length)")
    print(f"Time:   interpret YAML as UTC+{out['tz_offset_hours']} hours")
    print(f"Read packets: {out['packets_read']}")
    print(f"Matched:      {out['packets_matched']}")
    print()

    def print_stats(title: str, st: Dict[str, Any]):
        print(title)
        if st.get("count", 0) == 0:
            print("  count=0")
            return
        keys = [
            "count", "mean", "var", "std", "min", "q1", "median", "q3", "iqr",
            "p90", "p95", "p99", "max", "mad", "cv", "skew", "kurtosis_excess"
        ]
        line = "  " + "  ".join([f"{k}={fmt(st[k])}" for k in keys if k in st])
        print(line)

    print_stats("[OVERALL]", out["overall"])
    print()

    # Top labels
    print("[BY LABEL] (sorted by matched packet count desc)")
    for label, st in out["by_label"].items():
        print_stats(f"- {label}", st)


if __name__ == "__main__":
    main()
